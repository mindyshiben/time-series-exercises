{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb5c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9b8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "base_url = 'https://python.zgulde.net/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2d495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd81d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_items(resource):\n",
    "    \"\"\"Returns a list of dfs for each page that the api has\n",
    "    for a particular resource.\n",
    "    \n",
    "    resource = either 'items', 'stores', or 'sales'\n",
    "    \n",
    "    returns a list of dfs\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty list to store each page of data\n",
    "    dfs = []\n",
    "    base_url = 'https://python.zgulde.net/'\n",
    "\n",
    "    # just wanting to get max page to set the loop up for the \n",
    "    # correct number of iterations\n",
    "    response = requests.get(f'https://python.zgulde.net/api/v1/{resource}')\n",
    "    max_pages = response.json()['payload']['max_page']\n",
    "    \n",
    "    # loop for the amount of pages available\n",
    "    for x in range(1, max_pages+1):\n",
    "        \n",
    "        # just easier to read when making the endpoint url\n",
    "        page=x\n",
    "        \n",
    "        # create a generic url_endpoint that can be used with page\n",
    "        # number and resource type\n",
    "        url_endpoint = f'/api/v1/{resource}?page={page}'\n",
    "        response = requests.get(base_url + url_endpoint)\n",
    "        \n",
    "        # show what page we're on, and whether the response was ok\n",
    "        print(f'page {x}, response ok: {response.ok}')\n",
    "        \n",
    "        # append the df to a list of all dfs for this resource, \n",
    "        # to be concatenated later.\n",
    "        dfs.append(pd.DataFrame(response.json()['payload'][resource]))\n",
    "\n",
    "    # return the list of pages as dfs\n",
    "    return dfs\n",
    "\n",
    "def create_df(list_of_dfs):\n",
    "    \"\"\" creates a single df out of list of dfs\n",
    "    \n",
    "    list_of_dfs = a list of dfs to be concatenated vertically,\n",
    "    intended to be the output of the get_items function\"\"\"\n",
    "    \n",
    "    # instantiate empty df to be concatenated onto\n",
    "    total_df = pd.DataFrame()\n",
    "    \n",
    "    # concatenates each df from list to make a single\n",
    "    # df\n",
    "    for df in list_of_dfs:\n",
    "        total_df = pd.concat([total_df, df])\n",
    "\n",
    "    # returns single concatenated df\n",
    "    return total_df\n",
    "\n",
    "\n",
    "def all_resource_dfs(list_of_resources):\n",
    "    \"\"\"Uses a list of resources to request information to turn\n",
    "    into dataframes. A dictionary is made to hold the resulting \n",
    "    dfs, using the name of the resource as the key\n",
    "    \n",
    "    list_of_resources = list of resources to be acquired from this \n",
    "    specific api.\n",
    "    \"\"\"\n",
    "\n",
    "    # empty dict to hold all total dfs for every resource in\n",
    "    # the input list\n",
    "    all_dfs = {}\n",
    "\n",
    "    # gets list of dfs for each resource, then makes a total\n",
    "    # df for each resource. Entered into the dictionary\n",
    "    # using the resource name as key and the total df for that\n",
    "    # resource as the value.\n",
    "    for resource in list_of_resources:\n",
    "        resource_dfs = get_items(resource)\n",
    "        all_dfs[resource] = (create_df(resource_dfs))\n",
    "        \n",
    "    # returns total df for each resource\n",
    "    return all_dfs\n",
    "\n",
    "def resource_dfs_to_csv(resource_df_dict):\n",
    "    \"\"\"\n",
    "    Writes all dfs in the resource_df_dict to csv,\n",
    "    using the key of the entry as the file name.\n",
    "    resource_df_dict = dict holding different resources as keys\n",
    "    and the total df for that resource as a value.\n",
    "    \"\"\"\n",
    "    for key, value in resource_df_dict.items():\n",
    "        value.to_csv(f'{key}.csv')\n",
    "\n",
    "    sales = resource_df_dict['sales']\n",
    "    stores = resource_df_dict['stores']\n",
    "    items = resource_df_dict['items']\n",
    "\n",
    "    total = sales.merge(right=items, how='left', left_on='item', right_on='item_id' )\n",
    "    total = sales.merge(right=stores, how='left', left_on='store', right_on='store_id')\n",
    "\n",
    "    total.to_csv('total_df.csv')\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "def ignore_first(df):\n",
    "    '''Ignores first 'Unnamed: 0' column when reading csv'''\n",
    "    return df[df.columns[1:]]\n",
    "        \n",
    "def load_all_data():\n",
    "    \"\"\"return all three csvs\n",
    "    returns sales, stores, items\"\"\"\n",
    "    sales = ignore_first(pd.read_csv('sales.csv'))\n",
    "    stores = ignore_first(pd.read_csv('stores.csv'))\n",
    "    items = ignore_first(pd.read_csv('items.csv'))\n",
    "\n",
    "    return sales, stores, items\n",
    "\n",
    "def acquire_open_power_systems_data():\n",
    "    url = 'https://raw.githubusercontent.com/jenfly/opsd/master/opsd_germany_daily.csv'\n",
    "    return pd.read_csv(url)\n",
    "\n",
    "def acquire_sales_stores_items_data():\n",
    "    # list of all resources to make dfs out of \n",
    "    list_of_resources = ['items', 'stores', 'sales']\n",
    "    filename = 'total_df.csv'\n",
    "\n",
    "    if os.path.isfile(filename):\n",
    "        return ignore_first(pd.read_csv(filename))\n",
    "    else:\n",
    "        # create resource_dict to then be written to csv\n",
    "        resource_dict = all_resource_dfs(list_of_resources)\n",
    "        return resource_dfs_to_csv(resource_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcfa300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89eb67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa51b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
